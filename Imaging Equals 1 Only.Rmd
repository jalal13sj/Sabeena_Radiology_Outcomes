---
title: "247_data_analysis_for_Sabeena"
author: "Marshall"
date: '2019-05-02'
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,          # don't show code
  warning = FALSE,       # don't show warnings
  message = FALSE,       # don't show messages (less serious warnings)
  cache = FALSE,         # set to TRUE to save results from last compilation
  fig.align = "center",   # center figures
  fig.asp = 1,          # fig.aspect ratio
  fig.width = 5       # fig width
)

```

Summary

I've filtered out the imaging = 0 data.

Models: I'm having a tough time fitting the data to a model. Cox doesn't seem to work because it doesn't look that the hazards are proportional (hazard = instantaneous potential that the process is complete, ie: the end of mdex time). Weibell isn't appropriate either. I look at the data using Kaplan-Meier because that method doesn't require an underlying model (woohoo!). I also split mdex into tertiles and ran some ordinal regressions. I threw in some linear regressions at the end (not sure if they are appropriate for this kind of data).

Effect of group: it seems to be very small. Also, when looking at the raw mdex numbers and not adjusting, it seems that there are more instances of very small mdex observations in the pre 24/7 group than the post 24/7. Then for slightly larger mdex values, it flips. 

Sub-group analysis: It does seem like group has the strongest effect in the ctas1 and weakest in the ctas4 and 5.

```{r}
setwd("/Users/macbook/Documents/McGill School/Sabeena_Radiology_Outcomes")

#this data has the blank rows and "all-but-id" blank rows removed. It also has extrat binary colmns for the categorical variables for use in winbugs
wb_data <- read.csv("247_data_no_NA_GOOD.csv")
wb_data_na <- read.csv("247_data_some_NA_GOOD.csv")

wb_data$X <- NULL
wb_data_na$X <- NULL

#set factors to factors
wb_data$ctas1 <- as.factor(wb_data$ctas1)
wb_data$ctas2 <- as.factor(wb_data$ctas2)
wb_data$ctas3 <- as.factor(wb_data$ctas3)
wb_data$ctas4 <- as.factor(wb_data$ctas4)
wb_data$ctas5 <- as.factor(wb_data$ctas5)
wb_data$ctas <- as.factor(wb_data$ctas)
wb_data$todseen <- as.factor(wb_data$todseen)
wb_data$todseen1 <- as.factor(wb_data$todseen1)
wb_data$todseen2 <- as.factor(wb_data$todseen2)
wb_data$todseen3 <- as.factor(wb_data$todseen3)
wb_data$group <- as.factor(wb_data$group)
wb_data$imaging <- as.factor(wb_data$imaging)
wb_data$trauma <- as.factor(wb_data$trauma)
wb_data$adm <- as.factor(wb_data$adm)

#repeat with na
wb_data_na$ctas1 <- as.factor(wb_data_na$ctas1)
wb_data_na$ctas2 <- as.factor(wb_data_na$ctas2)
wb_data_na$ctas3 <- as.factor(wb_data_na$ctas3)
wb_data_na$ctas4 <- as.factor(wb_data_na$ctas4)
wb_data_na$ctas5 <- as.factor(wb_data_na$ctas5)
wb_data_na$ctas <- as.factor(wb_data_na$ctas)
wb_data_na$todseen <- as.factor(wb_data_na$todseen)
wb_data_na$todseen1 <- as.factor(wb_data_na$todseen1)
wb_data_na$todseen2 <- as.factor(wb_data_na$todseen2)
wb_data_na$todseen3 <- as.factor(wb_data_na$todseen3)
wb_data_na$group <- as.factor(wb_data_na$group)
wb_data_na$imaging <- as.factor(wb_data_na$imaging)
wb_data_na$trauma <- as.factor(wb_data_na$trauma)
wb_data_na$adm <- as.factor(wb_data_na$adm)

#this is where I can set the whole dang thing to with or without imaging
wb_data <- subset(wb_data, imaging == 1)
wb_data_na <- subset(wb_data_na, imaging == 1)
#nrow(wb_data_i1)
nrow(wb_data)
library(ggplot2)

```

Checking out the distribution of turn around time (mdex). 

```{r}

ggplot(data = wb_data_na, aes(x = mdex, color = group)) + 
  geom_histogram(binwidth = 200, alpha = 0.2, position = "identity", fill = "white") +
  labs(title = "Turn Around Time Distribution by Group", color = "Group", x = "Turn Around Time (minutes)", y = "Count") +
  scale_color_discrete(name = "Group", labels = c("pre-24/7","post 24/7"))
#looks like poisson, looks like group 1 and 2 have extreme observations

#look at the 0-1000 mdex range
ggplot(data = wb_data_na, aes(x = mdex, color = group)) + 
  geom_histogram(binwidth = 50, alpha = 0.2, position = "identity", fill = "white") + xlim(0, 1000) +
  labs(title = "Turn Around Time Distribution by Group", subtitle = "TAT < 1000 mins", color = "Group", x = "Turn Around Time (minutes)", y = "Count") +
  scale_color_discrete(name = "Group", labels = c("pre-24/7","post 24/7"))

ggplot(data = wb_data_na, aes(x = mdex, color = group)) + 
  geom_histogram(binwidth = 5, alpha = 0.2, position = "identity", fill = "white") + xlim(0, 100) +
  labs(title = "Turn Around Time Distribution by Group", subtitle = "TAT < 100 mins", color = "Group", x = "Turn Around Time (minutes)", y = "Count") +
  scale_color_discrete(name = "Group", labels = c("pre-24/7","post 24/7"))


#the 95th quantil is this
#quantile(wb_data_na$mdex, 0.95, na.rm = T)
#mean(wb_data_na$mdex, na.rm = T)
#sd(wb_data_na$mdex, na.rm = T)
#mean is smaller than the sd...check for overdispersion later


```

There are more very low values in the pre 24/7 group than in the post group. There are most moderately low values in the post group than in the pre group. If there is no confounding, then this suggests that the effect of the intervention switches (negative at very low mdex times and positive at moderately low mdex times). 

Now let's take a look at the TAT (mdex) in each group.
```{r}

ggplot(data = wb_data_na, aes(x = group, y = mdex)) + geom_boxplot() +
  labs(title = "Turn Around Time by Group", x = "Group", y = "Turn Around Time (minutes)") +
  scale_x_discrete(labels = c("pre 24/7", "post 24/7"))

ggplot(data = wb_data_na, aes(x = group, y = mdex)) + geom_boxplot(outlier.shape = NA) +
  scale_y_continuous(limits = c(0, 600)) +
  labs(title = "Turn Around Time by Group", subtitle = "Outliers Removed", x = "Group", y = "Turn Around Time (minutes)") +
  scale_x_discrete(labels = c("pre 24/7", "post 24/7"))

var(wb_data$mdex)
mean(wb_data$mdex)
```
It doesn't look like a huge difference between the groups. Note that the mean and variance are different. Likely dealing with overdispersion. 


**Table 1 Descriptives and Potential Confounding**
Let's look into potential confounders. See which variables appear to have a relationship with the intervention (group) AND with the outcome (turn around time, labelled "mdex").

Take a quick look at "Table 1" stratified by group to see if there are any imbalances of variables between the groups. An imbalance between groups would suggest a relationship with the group. 
```{r}
#install.packages("table1")
library(table1)


#manipulate data set for Table 1, wont do it now, but if the table should be nicer, then we can label all the categories
table1_data_na <- wb_data_na
table1_data_na$group <- factor(table1_data_na$group, levels = c(0,1), labels = c("pre 24/7", "post 24/7"))
table1_data_na$trauma <- factor(table1_data_na$trauma, levels = c(0,1), labels = c("Non-Trauma Pt", "Trauma Pt"))
table1_data_na$todseen <- factor(table1_data_na$todseen, levels = c(1,2,3), labels = c("Morning", "Evening", "Night"))
table1_data_na$adm <- factor(table1_data_na$adm, levels = c(0,1), labels = c("Not Admitted", "Admitted"))
table1_data_na$ctas <- factor(table1_data_na$ctas, levels = c(1,2,3,4,5), labels = c("Dead/Resus", 2, 3, 4, "Almost Normal"))

table1(~ ctas + adm + todseen + trauma + mdex | group, data = table1_data_na)


```
Everything looks pretty balanced. A couple of spots that have a difference of 1.6% between the groups, but most are less than 1%.
Side note: we can see the median and mean for each group. We can also see Where the missing data is. It's about 1.5% of the mdex, 1.5% of todseen, and basically 0% of the trauma.


Can sub-stratify by trauma and ctas if we want. I do that because those might be the least balanced of all the variables (I do proportion tests further down).
```{r}
table1(~ ctas  + adm + todseen + mdex | trauma*group, data = table1_data_na)

table1(~ trauma  + adm + todseen + mdex | ctas*group, data = table1_data_na)
```
Still looking very balanced. Note that this is a quick look and it tells us that none of the potential confounders has a super strong relationship with group. We can also see that the TAT (mdex) aren't hugely different between the groups either. We will come back to this later. 

When stratified by trauma: there seems to be less of an effect due to group. Notice how the mdex times actually go up post 24/7 group when trauma = 1. 

When stratified by ctas: the effect of group on mdex seems a bit more consistent, though the median increases post 24/7 for higher ctas levels. 


If we REALLY want to look hard, we can look at each level of each variable and see the propotion that is in each group. Perfect balance (and no relationship with group) would be 0.5, so we can do a bunch of tests of propotion vs the null of 0.5. I'm not sure this adds a lot of value since the n is so large, the CIs are very tight and some of them end up not covering the null even though the proportion is something like 0.51.
```{r}
#check columns proportions, not just overall
c.table <- table(table1_data_na$group, table1_data_na$ctas)
round(prop.table(c.table, 2), 3)
pt.ctas1 <- prop.test(c.table[2,1], sum(c.table[1:2,1]), p = 0.5)
pt.ctas2 <-  prop.test(c.table[2,2], sum(c.table[1:2,2]), p = 0.5)
pt.ctas3 <-  prop.test(c.table[2,3], sum(c.table[1:2,3]), p = 0.5)
pt.ctas4 <-  prop.test(c.table[2,4], sum(c.table[1:2,4]), p = 0.5)
pt.ctas5 <-  prop.test(c.table[2,5], sum(c.table[1:2,5]), p = 0.5)


to.table <- table(table1_data_na$group, table1_data_na$todseen)
round(prop.table(to.table, 2), 3)
pt.to1 <- prop.test(to.table[2,1], sum(to.table[1:2,1]), p = 0.5)
pt.tos2 <-  prop.test(to.table[2,2], sum(to.table[1:2,2]), p = 0.5)
pt.to3 <-  prop.test(to.table[2,3], sum(to.table[1:2,3]), p = 0.5)

a.table <- table(table1_data_na$group, table1_data_na$adm)
round(prop.table(a.table, 2), 3)
pt.a0 <- prop.test(a.table[2,1], sum(a.table[1:2,1]), p = 0.5)
pt.a1 <- prop.test(a.table[2,2], sum(a.table[1:2,2]), p = 0.5)

tr.table <- table(table1_data_na$group, table1_data_na$trauma)
round(prop.table(tr.table, 2), 3)
pt.tr0 <- prop.test(tr.table[2,1], sum(tr.table[1:2,1]), p = 0.5)
pt.tr1 <- prop.test(tr.table[2,2], sum(tr.table[1:2,2]), p = 0.5)
```


Now look at the CIs of the prop tests. These are the proportion of a certain level of a variable that is in group 1 (ie: the post 24/7 group). This is showing the min and max values of the proportion in group 1 (for each level of variable) that are compatible with our data. 

```{r}
ctas.pt.cis <- data.frame(row.names = colnames(wb_data[c(9:13)]), round(matrix(data = c(pt.ctas1$conf.int, pt.ctas2$conf.int, pt.ctas3$conf.int, pt.ctas4$conf.int, pt.ctas5$conf.int), nrow = 5, ncol = 2, byrow = TRUE), 3))
colnames(ctas.pt.cis) <- c("2.5%", "97.5%")
ctas.pt.cis

to.pt.cis <- data.frame(row.names = colnames(wb_data[c(14:16)]), round(matrix(data = c(pt.to1$conf.int, pt.tos2$conf.int, pt.to3$conf.int), nrow = 3, ncol = 2, byrow =TRUE), 3))
colnames(to.pt.cis) <- c("2.5%", "97.5%")
to.pt.cis

a.pt.cis <- data.frame(row.names = c("adm0", "adm1"), round(matrix(data = c(pt.a0$conf.int, pt.a1$conf.int), nrow = 2, ncol = 2, byrow =TRUE), 3))
colnames(a.pt.cis) <- c("2.5%", "97.5%")
a.pt.cis

tr.pt.cis <- data.frame(row.names = c("trauma0", "trauma1"), round(matrix(data = c(pt.tr0$conf.int, pt.tr1$conf.int), nrow = 2, ncol = 2, byrow =TRUE), 3))
colnames(tr.pt.cis) <- c("2.5%", "97.5%")
tr.pt.cis


```
Only CTAS and Trauma have levels where CIs go beyond 2% fromm 0.5. This suggests there may be some sort of relationship between group and CTAS and Trauma, but not a strong one. Notice ctas1 has a CI that goes up to 0.542, this is due to small numbers of ctas1 (this is widest of all CIs).

Todseen, imaging, and adm have at most 1.5% from 0.5 and many levels have chunks of CI that cross or are very close to 0.5. 

I honnestly don't think any of these variables have an important relationship with group. If I saw these numbers in an RCT, I wouldn't bat an eyelash and would be believe that randomization worked. If we really want to force ourselves to think that there might be confounding, maybe trauma and ctas have some sort of relationship with group (ie: imbalance). 

Group is pre vs post 24/7, is there some reason to think that there is a relationship between group and any of the indepenant variables? Eg: once the 24/7 was implemented, more serious cases were sent to that hospital?

Now we can look at the relationship between the variables and outcome. 
```{r}
boxplot(wb_data$mdex ~ wb_data$trauma, xlab = "trauma", ylab = "mdex")
boxplot(wb_data$mdex ~ wb_data$adm, xlab = "adm", ylab = "mdex")
boxplot(wb_data$mdex ~ wb_data$todseen, xlab = "toseen", ylab = "mdex")
boxplot(wb_data$mdex ~ wb_data$ctas, xlab = "ctas", ylab = "mdex")
boxplot(wb_data$mdex ~ wb_data$imaging, xlab = "imaging", ylab = "mdex")
```

That's a bit hard to see with the outliers, so take them out:
```{r}
boxplot(wb_data$mdex ~ wb_data$trauma, xlab = "trauma", ylab = "mdex", outline = F)
boxplot(wb_data$mdex ~ wb_data$adm, xlab = "adm", ylab = "mdex", outline = F)
boxplot(wb_data$mdex ~ wb_data$todseen, xlab = "toseen", ylab = "mdex", outline = F)
boxplot(wb_data$mdex ~ wb_data$ctas, xlab = "ctas", ylab = "mdex", outline = F)
boxplot(wb_data$mdex ~ wb_data$imaging, xlab = "imaging", ylab = "mdex", outline = F)
```
They all seem to have some sort of relationship with the outcome. Adm, CTAS, and trauma have the strongest relationships. That is important to keep in mind. Later, we will see that group has a very weak relationship with mdex, so does that mean we should be hyper-vigilant to even the weakest confounders? If there is evidence that adm, ctas, and trauma have weak relationships with group but strong relationships with the mdex, then maybe we should be concerned that even that weak confounding could distort the relationship between group and mdex due to that relationship being so weak and vulnerable to confounding. When I talked to Dr Jospeh, he said not really. He said that it doesn't look like there is much confounding.

Before we look at regressions, lets see if some of the determinants are correlated with each other. 

```{r}
table1(~ ctas | trauma, data = table1_data_na)
table1(~ trauma | ctas, data = table1_data_na)
```
Would you look at that! Actually pretty balanced (look at overall distribution and see how each one compares). That means it's hard to predict trauma level based on ctas level. That means less correlated. Or does it? I'm confused. 

```{r}
table1(~ ctas | adm, data = table1_data_na)
table1(~ adm | ctas, data = table1_data_na)

```
Less balanced. That means we can semi-predict whether they were admited based on CTAS. I think this means that if we look at Adm or CTAS, we are semi-gretting the same information. 

```{r}
table1(~ trauma | adm, data = table1_data_na)
table1(~ adm | trauma, data = table1_data_na)
```
So-so. 


Let's do some univariate regressions, and then do multivariate with Trauma, CTAS, Adm, and Group. Which regression? Poisson would only be appropriate if we transformed the data to counts per time. Dr Jospeh suggested Cox or Weibell. 

**Cox Semi-Parametric Regression**
```{r}

library(survival)
surv.mdex <- Surv(wb_data$mdex)

#cox semis, uni, all multi, and just ctas multi
g.uni.cox <- coxph(data = wb_data, formula = Surv(mdex) ~ group)
summary(g.uni.cox)
#2.7% to 5.5% faster

all.multi.cox <- coxph(surv.mdex ~ wb_data$group + wb_data$ctas + wb_data$trauma + wb_data$todseen + wb_data$adm)
summary(all.multi.cox)

g.c.multi.cox <- coxph(surv.mdex ~ wb_data$group + wb_data$ctas)
summary(g.c.multi.cox)
#can also stratify by ctas, that just gives us one coefficient for group, none for ctas
coxph(surv.mdex ~ wb_data$group + strata(wb_data$ctas))

#if we want to look at sub group analysis, or actually interaction terms
g.c.int.multi.cox <- coxph(data = wb_data, formula = Surv(mdex) ~ group + group*ctas)
summary(g.c.int.multi.cox)
#how is group*ctas1 different than ctas1? Check the interactions of two categorical notes. Oh yeah, ctas on it's own is the effect of ctas regardless of group, ctas*group is the effect of ctas in group 1 (or vice versa, their effect together when both present)
  #This output tells us that the biggest group effect is in ctas1 (6.7%), then ctas2 and 3 are pretty big (5.4% and 5.9%), then ctas4 sees hardly a group effect, and ctas5 is 2%

all.int.multi.cox <- coxph(data = wb_data, formula = Surv(mdex) ~ group + group*trauma + group*adm)
summary(all.int.multi.cox)
```
That was fun! But before I get too excited, I should probably check the proportional hazards assumption **ominous music play**


```{r}
g.cox.test <- cox.zph(g.uni.cox)
g.cox.test
cox.zph(g.c.multi.cox)
cox.zph(all.multi.cox)
g.c.int.cox.test <- cox.zph(g.c.int.multi.cox)
g.c.int.cox.test

library(survminer)

ggcoxzph(g.cox.test)

lm(data = wb_data, mdex ~ group)

result.surv.g0 <- survfit(Surv(wb_data$mdex) ~ wb_data$group, subset = {wb_data$group == 0})
time.g0 <- result.surv.g0$time
surv.g0 <- result.surv.g0$surv
cloglog.g0 <- log(-log(surv.g0))
logtime.g0 <- log(time.g0)

result.surv.g1 <- survfit(Surv(wb_data$mdex) ~ wb_data$group, subset = {wb_data$group == 1})
time.g1 <- result.surv.g1$time
surv.g1 <- result.surv.g1$surv
cloglog.g1 <- log(-log(surv.g1))
logtime.g1 <- log(time.g1)

plot(cloglog.g0 ~ logtime.g0, type="s", col="blue", lwd=2)
lines(cloglog.g1 ~ logtime.g1, col="red", lwd=2, type="s")
legend("bottomright", legend=c("Pre 24/7", "Post 24/7"), col=c("blue","red"), lwd=2)
#they cross a lot. Would need to allow it to vary with time.

#looking for time variable, just following the textbook
#group.n <- rep(0, nrow(wb_data))
#group.n[wb_data$group == "1"] <- 1
#result.247 <- coxph(Surv(wb_data$mdex) ~ group.n)
#result.247

#result.247.tt <- coxph(Surv(wb_data$mdex) ~ group.n + tt(group.n), tt=function(x,t, ...) x*log(t))
#result.247.tt
#gives me an error for infinity

#group.n <- rep(0, nrow(data_under_1000))
#group.n[data_under_1000$group == "1"] <- 1
#result.247 <- coxph(Surv(data_under_1000$mdex) ~ group.n)
#result.247

#result.247.tt <- coxph(Surv(data_under_1000$mdex) ~ group.n + tt(group.n), tt=function(x,t, ...) x*log(t))
#result.247.tt
#still can't get it to work, no error, but takes more than 5 minutes to run. Maybe try again later. 


#Here's if we want to check with covariates, I tried with a coulple and it still comes out super crossy
#result.surv.g0 <- survfit(Surv(wb_data$mdex) ~ wb_data$group + wb_data$adm, subset = {wb_data$group == 0})
#time.g0 <- result.surv.g0$time
#surv.g0 <- result.surv.g0$surv
#cloglog.g0 <- log(-log(surv.g0))
#logtime.g0 <- log(time.g0)

#result.surv.g1 <- survfit(Surv(wb_data$mdex) ~ wb_data$group + wb_data$adm, subset = {wb_data$group == 1})
#time.g1 <- result.surv.g1$time
#surv.g1 <- result.surv.g1$surv
#cloglog.g1 <- log(-log(surv.g1))
#logtime.g1 <- log(time.g1)

#plot(cloglog.g0 ~ logtime.g0, type="s", col="blue", lwd=2)
#lines(cloglog.g1 ~ logtime.g1, col="red", lwd=2, type="s")
#legend("bottomright", legend=c("Pre 24/7", "Post 24/7"), col=c("blue","red"), lwd=2)

#maybe check from plots for linearity.

```
Cox assumes proportional hazards. We might not be able to assume this. 

The coxzph() function for each regression gives p values below 0.05, which means the hazard functions are not parrallel (ie: not proportional).

The graph shows them crossing about a billion times. Actually only three times. That's bad. When I follow a method I found in a textbook to determine the time varying hazard ratio, my R either has an "inf" error or works really hard and never finds something. I'm guessing it's because it's trying to estimate a higher order polynomial (recall the lines cross 3 times) and it's just too hard. The time varying hazard ratio would have to switch signs 4 times. That makes me suspicious. Would that be overfitting? Or trying to fit to unmeasured confounders? It might be that the covariates are much stronger determinants than group. Also, if I step back and think about it, the lines cross so much because they are so close together. They are nearly parrallel, but they cross because the effect of group is so weak. 

I also did a cloglog vs logtime plot of each individual ctas level (ie: subgroup check of porportional hazards) and they were equally messy and crossy.

I could also do some plots to check the linearity, but I'll hold off for now since the hazards are so clearly no proportional. 

**Weibel exploration**
```{r}
library(Hmisc)
weib.km <- survfit(Surv(wb_data$mdex) ~ 1)

survEst <- weib.km$surv
survTime <- weib.km$time 
logLogSurvEst <- log(-log(survEst)) 
logSurvTime <- log(survTime)

logSurvTime <- ifelse(logSurvTime == -Inf, 0, logSurvTime)
logLogSurvEst <- ifelse(logLogSurvEst == Inf, 2.5, logLogSurvEst)
describe(logSurvTime)
describe(logLogSurvEst)

plot(logLogSurvEst ~ logSurvTime)
result.lm <- lm(logLogSurvEst ~ logSurvTime) 
abline(result.lm)
#This shows that a weibel is not apporpriate.

```
Weibell is not appropriate. That plot would need to be linear for Weibell to be appropriate. 

So those two models don't work. Maybe that means no model. We can look at it in Kaplan-Meier. KM does not have an underlying model, which is nice! The drawback is that KM can only tell us about the difference, and cannot really tell us about the magnitude of the difference. 


```{r}

#km
g.km <- survfit(surv.mdex ~ wb_data$group)
plot(g.km, xlab="Time in Minutes", ylab="Process Completion Probability", col=c("blue", "red"), lwd=2)
legend("topright", legend=c("Pre 24/7", "Post 24/7"), col=c("blue","red") , lwd=2)
title("K-M for mdex underfrom entire dataset")

survdiff(Surv(wb_data$mdex) ~ wb_data$group)
#congrats, there's a difference, but what's the magnitude? That's the limitation of KM

data_under_2000 <- subset(wb_data, mdex < 2000)

g.km.2000 <- survfit(Surv(data_under_2000$mdex) ~ data_under_2000$group)
plot(g.km.2000, xlab="Time in Minutes", ylab="Process Completion Probability", col=c("blue", "red"), lwd=2)
legend("topright", legend=c("Pre 24/7", "Post 24/7"), col=c("blue","red") , lwd=2)
title("K-M for mdex under 2000 minutes")

data_under_1000 <- subset(wb_data, mdex < 1000)

g.km.1000 <- survfit(Surv(data_under_1000$mdex) ~ data_under_1000$group)
plot(g.km.1000, xlab="Time in Minutes", ylab="Process Completion Probability", col=c("blue", "red"), lwd=2)
legend("topright", legend=c("Pre 24/7", "Post 24/7"), col=c("blue","red") , lwd=2)
title("K-M for mdex under 1000 minutes")
```
You can see on the full data set and the "under 2000 minutes" data set that the red line dips below the blue line between roughly 500 and 1500 minutes. That means that in that region, the post 24/7 is faster. However, for the "under 500 minutes" dataset, the blue line is below the red line, which means that the pre 24/7 is faster. This is compatible with the first couple of histograms, we see that the pre 24/7 group has more observations with very small mdex. 

We can look as K-M curves for each level of CTAS:
```{r}

data_ctas1 <- subset(wb_data, ctas ==1)
data_ctas2 <- subset(wb_data, ctas ==2)
data_ctas3 <- subset(wb_data, ctas ==3)
data_ctas4 <- subset(wb_data, ctas ==4)
data_ctas5 <- subset(wb_data, ctas ==5)

g.km.ctas1 <- survfit(Surv(data_ctas1$mdex) ~ data_ctas1$group)
plot(g.km.ctas1, xlab="Time in Minutes", ylab="Process Completion Probability", col=c("blue", "red"), lwd=2)
legend("topright", legend=c("Pre 24/7", "Post 24/7"), col=c("blue","red") , lwd=2)
title("K-M for mdex of CTAS1")

g.km.ctas2 <- survfit(Surv(data_ctas2$mdex) ~ data_ctas2$group)
plot(g.km.ctas2, xlab="Time in Minutes", ylab="Process Completion Probability", col=c("blue", "red"), lwd=2)
legend("topright", legend=c("Pre 24/7", "Post 24/7"), col=c("blue","red") , lwd=2)
title("K-M for mdex of CTAS2")

g.km.ctas3 <- survfit(Surv(data_ctas3$mdex) ~ data_ctas3$group)
plot(g.km.ctas3, xlab="Time in Minutes", ylab="Process Completion Probability", col=c("blue", "red"), lwd=2)
legend("topright", legend=c("Pre 24/7", "Post 24/7"), col=c("blue","red") , lwd=2)
title("K-M for mdex of CTAS3")

g.km.ctas4 <- survfit(Surv(data_ctas4$mdex) ~ data_ctas4$group)
plot(g.km.ctas4, xlab="Time in Minutes", ylab="Process Completion Probability", col=c("blue", "red"), lwd=2)
legend("topright", legend=c("Pre 24/7", "Post 24/7"), col=c("blue","red") , lwd=2)
title("K-M for mdex of CTAS4")

g.km.ctas5 <- survfit(Surv(data_ctas5$mdex) ~ data_ctas5$group)
plot(g.km.ctas4, xlab="Time in Minutes", ylab="Process Completion Probability", col=c("blue", "red"), lwd=2)
legend("topright", legend=c("Pre 24/7", "Post 24/7"), col=c("blue","red") , lwd=2)
title("K-M for mdex of CTAS5")

```
A quick visual inspection shows CTAS2 and CTAS3 have the clearest benefits of 24/7. CTAS1 has the switching of effect and CTAS4 and 5 are less clear. Maybe take a look at trunkated data with them as well (ie: less than 1000 and 2000).


```{r}

data_ctas1_under1000 <- subset(data_under_1000, ctas ==1)
data_ctas2_under1000 <- subset(data_under_1000, ctas ==2)
data_ctas3_under1000 <- subset(data_under_1000, ctas ==3)
data_ctas4_under1000 <- subset(data_under_1000, ctas ==4)
data_ctas5_under1000 <- subset(data_under_1000, ctas ==5)

g.km.ctas1.u1000 <- survfit(Surv(data_ctas1_under1000$mdex) ~ data_ctas1_under1000$group)
plot(g.km.ctas1, xlab="Time in Minutes", ylab="Process Completion Probability", col=c("blue", "red"), lwd=2)
legend("topright", legend=c("Pre 24/7", "Post 24/7"), col=c("blue","red") , lwd=2)
title("K-M for mdex of CTAS1 under 1000 minutes")

g.km.ctas2.u1000  <- survfit(Surv(data_ctas2_under1000$mdex) ~ data_ctas2_under1000$group)
plot(g.km.ctas2, xlab="Time in Minutes", ylab="Process Completion Probability", col=c("blue", "red"), lwd=2)
legend("topright", legend=c("Pre 24/7", "Post 24/7"), col=c("blue","red") , lwd=2)
title("K-M for mdex of CTAS2 under 1000 minutes")

g.km.ctas3.u1000  <- survfit(Surv(data_ctas3_under1000$mdex) ~ data_ctas3_under1000$group)
plot(g.km.ctas3, xlab="Time in Minutes", ylab="Process Completion Probability", col=c("blue", "red"), lwd=2)
legend("topright", legend=c("Pre 24/7", "Post 24/7"), col=c("blue","red") , lwd=2)
title("K-M for mdex of CTAS3 under 1000 minutes")

g.km.ctas4.u1000  <- survfit(Surv(data_ctas4_under1000$mdex) ~ data_ctas4_under1000$group)
plot(g.km.ctas4, xlab="Time in Minutes", ylab="Process Completion Probability", col=c("blue", "red"), lwd=2)
legend("topright", legend=c("Pre 24/7", "Post 24/7"), col=c("blue","red") , lwd=2)
title("K-M for mdex of CTAS4 under 1000 minutes")

g.km.ctas5.u1000  <- survfit(Surv(data_ctas5_under1000$mdex) ~ data_ctas5_under1000$group)
plot(g.km.ctas4, xlab="Time in Minutes", ylab="Process Completion Probability", col=c("blue", "red"), lwd=2)
legend("topright", legend=c("Pre 24/7", "Post 24/7"), col=c("blue","red") , lwd=2)
title("K-M for mdex of CTAS5 under 1000 minutes")
```
If we "zoom in" on the mdex under 1000 for each level of ctas, we can still see the lue line dipping below the erd. Maybe less pronounced than before, but it is still there. 


Subgroup of Adm:
```{r}

data_adm0 <- subset(wb_data, adm ==0)
data_adm1 <- subset(wb_data, adm ==1)

data_adm0_under1000 <- subset(data_under_1000, adm ==0)
data_adm1_under1000 <- subset(data_under_1000, adm ==1)


g.km.adm0 <- survfit(Surv(data_adm0$mdex) ~ data_adm0$group)
plot(g.km.adm0, xlab="Time in Minutes", ylab="Process Completion Probability", col=c("blue", "red"), lwd=2)
legend("topright", legend=c("Pre 24/7", "Post 24/7"), col=c("blue","red") , lwd=2)
title("K-M for mdex of Adm = 0 all data")

g.km.adm0.u1000 <- survfit(Surv(data_adm0_under1000$mdex) ~ data_adm0_under1000$group)
plot(g.km.adm0.u1000, xlab="Time in Minutes", ylab="Process Completion Probability", col=c("blue", "red"), lwd=2)
legend("topright", legend=c("Pre 24/7", "Post 24/7"), col=c("blue","red") , lwd=2)
title("K-M for mdex of Adm = 0 under 1000 minutes")

g.km.adm1 <- survfit(Surv(data_adm1$mdex) ~ data_adm1$group)
plot(g.km.adm1, xlab="Time in Minutes", ylab="Process Completion Probability", col=c("blue", "red"), lwd=2)
legend("topright", legend=c("Pre 24/7", "Post 24/7"), col=c("blue","red") , lwd=2, box.lty=0)
title("K-M for mdex of Adm = 1 all data")

g.km.adm1.u1000 <- survfit(Surv(data_adm1_under1000$mdex) ~ data_adm1_under1000$group)
plot(g.km.adm1.u1000, xlab="Time in Minutes", ylab="Process Completion Probability", col=c("blue", "red"), lwd=2)
legend("topright", legend=c("Pre 24/7", "Post 24/7"), col=c("blue","red") , lwd=2)
title("K-M for mdex of Adm = 1 under 1000 minutes")

```
Blue is below for Adm = 0 even when look at all the data (though that is very close). Red is below for both plots of Adm = 1. WHAT DOES THIS MEAN?!?! Pre 24/7 is faster when Adm = 0 and post 24/7 is faster when Adm = 1?


Subgroup of Trauma:
```{r}
data_tr0 <- subset(wb_data, trauma ==0)
data_tr1 <- subset(wb_data, trauma ==1)

data_tr0_under1000 <- subset(data_under_1000, trauma ==0)
data_tr1_under1000 <- subset(data_under_1000, trauma ==1)


g.km.tr0 <- survfit(Surv(data_tr0$mdex) ~ data_tr0$group)
plot(g.km.tr0, xlab="Time in Minutes", ylab="Process Completion Probability", col=c("blue", "red"), lwd=2)
legend("topright", legend=c("Pre 24/7", "Post 24/7"), col=c("blue","red") , lwd=2)
title("K-M for mdex of Trauma = 0 all data")

g.km.tr0.u1000 <- survfit(Surv(data_tr0_under1000$mdex) ~ data_tr0_under1000$group)
plot(g.km.tr0.u1000, xlab="Time in Minutes", ylab="Process Completion Probability", col=c("blue", "red"), lwd=2)
legend("topright", legend=c("Pre 24/7", "Post 24/7"), col=c("blue","red") , lwd=2)
title("K-M for mdex of Trauma = 0 under 1000 minutes")

g.km.tr1 <- survfit(Surv(data_tr1$mdex) ~ data_tr1$group)
plot(g.km.tr1, xlab="Time in Minutes", ylab="Process Completion Probability", col=c("blue", "red"), lwd=2)
legend("topright", legend=c("Pre 24/7", "Post 24/7"), col=c("blue","red") , lwd=2, box.lty=0)
title("K-M for mdex of Trauma = 1 all data")

g.km.tr1.u1000 <- survfit(Surv(data_tr1_under1000$mdex) ~ data_tr1_under1000$group)
plot(g.km.tr1.u1000, xlab="Time in Minutes", ylab="Process Completion Probability", col=c("blue", "red"), lwd=2)
legend("topright", legend=c("Pre 24/7", "Post 24/7"), col=c("blue","red") , lwd=2)
title("K-M for mdex of Trauma = 1 under 1000 minutes")

```
Trauma is like ctas, it is red below in the 500 - 2000 minute range. Then when we just look at the 1000 fasted minutes, the blue dips below in the under 500 minute area. 



Just looking at plain old linear regressions. Not sure if they are appropriate. Need to look more into this and check the assumptions. The distribution of mdex is clearly not normal (wicked long tail like a jaguar), but maybe there are enough observations for us to feel comfortable that the CLT has kicked in?
```{r}
g.uni <- lm(data = wb_data, mdex ~ group)
confint(g.uni)

g.c.int.multi <- lm(data = wb_data, mdex ~ group + group*ctas)
confint(g.c.int.multi)
```
With only group, the model describes the effect of group at -17 minutes (ie: 17 minutes faster in post 24/7).

With group and a ctas interaction term, it shows that group has the greatest absolute effect on the ctas1 level (-34 mins). It shows that group has an absolute effect of -28 mins in ctas2 and -22 minutes in ctas3. Not much of an effect in ctas4 or ctas5. The CIs are very wide and all of them cross the null. 

Another way to do subgroup analysis is to do linear regressions on each subset of the data.
```{r}
lm(data = subset(wb_data, ctas == 1), mdex ~ group)
lm(data = subset(wb_data, ctas == 2), mdex ~ group)
lm(data = subset(wb_data, ctas == 3), mdex ~ group)
lm(data = subset(wb_data, ctas == 4), mdex ~ group)
lm(data = subset(wb_data, ctas == 5), mdex ~ group)

confint(lm(data = subset(wb_data, ctas == 3), mdex ~ group))
```
You can see this gives the same Point Estimates as the single regression with interaction terms (ie: the group coefficient plus each specific interaction term for the subgroup of interest). Note that the confidence interval is much more narrow with the individuals linear regressions than it is for the interaction term regression. I'm not sure which one is a more accurate representation of the uncertainty, but my instinct is that the wider CIs of the regression with interaction terms is a more honnest/accurate representation of the uncertainty.

Can check some other multi regressions and the bic.glm for fun. Further below I check the assumptions 
```{r}
library(MASS)

lm(data = wb_data, mdex ~ group + ctas + trauma + adm)
summary(lm(data = wb_data, mdex ~ group + ctas + trauma + adm + todseen))
confint(lm(data = wb_data, mdex ~ group + ctas + trauma + adm))

bic.linear <- bic.glm.formula(data = wb_data, f = mdex ~ group + ctas + trauma + adm + todseen, glm.family = gaussian())
summary(bic.linear)
```
"Full up" model has group effect at -11 minutes. The full model has a higher R (obvously), but recall that's not the aim. 

Checking assumptions of linear regressions
```{r}

uni.resid <- data.frame(g.uni$residuals, wb_data$group)
head(uni.resid)

ggplot(data = uni.resid, aes(x = g.uni.residuals)) + 
  geom_histogram() + xlim(0, 4000) +
  labs(title = "Univariate Linear Regression Residuals", x = "lm(mdex ~ group) Residuals", y = "Count")


int.resid <- data.frame(g.c.int.multi$residuals, wb_data$group)
head(int.resid)

ggplot(data = int.resid, aes(x = g.c.int.multi.residuals)) + 
  geom_histogram() + xlim(0, 4000) +
  labs(title = "Multivariate Interaction Terms Linear Regression Residuals", x = "lm(mdex ~ group + group*ctas) Residuals", y = "Count")

plot(wb_data$group, uni.resid$g.uni.residuals)

```

Relationship is linear between X and Y , i.e., relation is a straight line: Dummy variables (ie: categorical) meet the assumption of linearity by definition, because they creat two data points, and two points define a straight line. There is no such thing as a non-linear relationship for a single variable with only two values. Dummy variables need no linearity assumptions, as they are already linear.

The “errors” (also known as “residuals”) are independent N(0, σ2): Not very normal at all. See the histograms above. 

σ2 is constant throughout the range: Not sure how to check with with dummy variables. 

Conclusion: We can probably assume the CLT has kicked in and the linear regression is pretty robust. That being said, the residuals aren't normally distributed. We can take the information from the linear regression with a grain of salt. We see some effect due to group and it appears to interact with CTAS. We shouldn't get too excited about that precision. Maybe we can look at other models to see what they say. 

Could look at doing a log transformation of mdex. This would be a log-linear model. 
```{r}
head(wb_data)
wb_data$log.mdex <- log(wb_data$mdex)
log(1)

histogram(data = wb_data, wb_data$log.mdex)

#there are some zero values for mdex. See if I can substitute 1 for zero. 
wb_data$log.mdex <- ifelse(wb_data$log.mdex == -Inf, 0, wb_data$log.mdex)

describe(wb_data$log.mdex)

g.log.uni <- lm(data = wb_data, formula = log.mdex ~ group)
g.log.uni
summary(g.log.uni)
exp(g.log.uni$coefficients)
exp(confint(g.log.uni))
#is the intercept allowed to be that different from the actual mean and median? Is that a by-product of the model fit or something?
#notice the summary of the R2, which is attrocious. I guess that would be expected. The "explanatory" variable of group is super weak and explains very little of the variance. Recall that the aim of this exercise is to see what efect group has, not to predict outcome. 

uni.log.resid <- data.frame(g.log.uni$residuals, wb_data$group)
head(uni.log.resid)

ggplot(data = uni.log.resid, aes(x = g.log.uni.residuals)) + 
  geom_histogram() +
  labs(title = "Univariate Log-Linear Regression Residuals", x = "lm(log(mdex) ~ group) Residuals", y = "Count")
#well shit, that loooks great! This satisfies the assumption that the errors are independent

plot(wb_data$group, g.log.uni$residuals)
#this looks good as well. The determinant is categorical, so we are just for the spread to be similar for each level. This looks to be satisfied. The variance appears to be constant accross the range. 

```
Now the 3 assumptions are much more respected (see the graphs). In this case, group doesn't seem to have an effect. 

Just messing
```{r}
d.frame <- data.frame(y = c(exp(rnorm(40, mean = 5, sd = 1)), exp(rnorm(40, mean = 6, sd = 1))), x = c(rep(1, 40), rep(0, 40)))
d.frame
#the mean of group 0 is 
mean(d.frame$y)
mean(subset(d.frame, x == 1)$y)
#240
log(240)

mean(subset(d.frame, x == 0)$y)
#44647
log(44647)

44647/240


plot(d.frame$x, d.frame$y)
histogram(d.frame$y, xlim = c(0, 1000), breaks = 200)
histogram(log(d.frame$y))

ggplot(data = d.frame, aes(x = y)) + geom_histogram()
ggplot(data = d.frame, aes(x = log(y))) + geom_histogram()

lm.mss <- lm(data = d.frame, y ~ x)
lm.mss
histogram(lm.mss$residuals)


log.lm.mss <- lm(data = d.frame, log(y) ~ x)
log.lm.mss
histogram(log.lm.mss$residuals)
#so the intercept here is the center of the normal dist of the numbers, it's not the average number. Is this because the actual data peaks around 100, wearas an exponential curve goes to infinity? That means an exponential curve has a lot more small numbers than the actual data here so when it tries to estimate the average it goes too low? Maybe. Still, the important thing is the beta, that is the aim of this analysis. \
exp(log.lm.mss$coefficients)
exp(confint(log.lm.mss))
exp(-5.451)*44647
0.3978579*694

#this is what the mean should be for group 0
exp(5.9736)

```

Now look at log-linear with a ctas interaction term:
```{r}

g.log.ctas.int <- lm(data = wb_data, formula = log.mdex ~ group + ctas*group)
g.log.ctas.int
summary(g.log.ctas.int)
exp(g.log.ctas.int$coefficients)
exp(confint(g.log.ctas.int))

log.int.resid <- data.frame(g.log.ctas.int$residuals, wb_data$group)
head(log.int.resid)

ggplot(data = log.int.resid, aes(x = g.log.ctas.int.residuals)) + 
  geom_histogram() + 
  labs(title = "Multivariate Interaction Terms Linear Regression Residuals", x = "lm(mdex ~ group + group*ctas) Residuals", y = "Count")

plot(wb_data$group, log.int.resid$g.log.ctas.int.residuals)
#another good looking couple of plots. 

```
Like in the linear interaction model, group seems to have the stronget effect in CTAS1 (3.8%) and some effect in CTAS2 and 3 (1-2%). Not much of an effect/reverse effect in CTAS4 and 5. Keep in mind, the CIs are all wide and cover the null. All inconclusive, though there may be some interesting effects in CTAS1, and possibly in 2 and 3 (though evidence in 2 and 3 is much less strong).  

Shotgun a bunch of other models and do a bic.
```{r}
bic.log.linear <- bic.glm.formula(data = wb_data, f = log.mdex ~ group + ctas + trauma + adm + todseen, glm.family = gaussian())
summary(bic.log.linear)
exp(bic.log.linear$mle)

g.log.all.multi <- lm(data = wb_data, formula = log.mdex ~ group + ctas + trauma + adm + todseen)
g.log.all.multi
summary(g.log.all.multi)
exp(g.log.all.multi$coefficients)
exp(confint(g.log.all.multi))

g.log.adm.int <- lm(data = wb_data, formula = log.mdex ~ group + group*adm)
g.log.adm.int
summary(g.log.adm.int)
exp(g.log.adm.int$coefficients)
exp(confint(g.log.adm.int))
1.079*0.8559

g.log.adm.multi <- lm(data = wb_data, formula = log.mdex ~ group + adm)
g.log.adm.multi
summary(g.log.adm.multi)
exp(g.log.adm.multi$coefficients)
exp(confint(g.log.adm.multi))


```
The bic keeps group in and says it actaully makes the time slower. The "everything in" also says that group makes it slower. I don't really think these are good models for describing the effect of group, but there they are. 

Doing an interaction term with adm shows that when not admied, the group makes it slower. When admitted, the group makes it faster. 



We can turn mdex into tertile categorical and run an ordinal regression.
```{r}

#cut off for the first tertile
quantile(wb_data$mdex, probs = 1/3)
#cut off for the second tertile
quantile(wb_data$mdex, probs = 2/3)

wb_data$mdex.tert <- cut(wb_data$mdex, breaks = c(-Inf, quantile(wb_data$mdex, probs = 1/3), quantile(wb_data$mdex, probs = 2/3), Inf), labels = c("1","2","3"))

g.uni.ord <- polr(data = wb_data, mdex.tert ~ group)
coef(g.uni.ord)
g.uni.ord.cis <- exp(confint(g.uni.ord))

g.c.multi.int.ord <- polr(data = wb_data, mdex.tert ~ group + group*ctas)
summary(g.c.multi.int.ord)
g.c.int.cis <- exp(confint(g.c.multi.int.ord))


multi.ord <- polr(data = wb_data, mdex.tert ~ group + ctas + adm + trauma)
coef(g.uni.ord)
multi.ord.cis <- exp(confint(multi.ord))

quantile(wb_data$mdex, probs = c(1/3, 2/3))
quantile(wb_data$mdex, probs = 0.95)



```
The cut off between each tertile is 164 minutes and 374 minutes.

Here is a univariate ordinal with just group:
```{r}
g.uni.ord.cis
```
I think this is saying that the intervention makes it more likely to take longer (ie: be in a higher mdex tertile).

Here is looking at group and the interaction with ctas (ie: how ctas modifies the effect of group, I think this is basically a subgroup analysis):
```{r}
g.c.int.cis

```
The CI is wide and covers null, but suggests that the intervention makes it more likely to take less time in ctas1. The effect is  less pronounced in each of the other ctas.

No interaction, just all possible covariates included:
```{r}
multi.ord.cis
```
Here again has the intervention making it more likely to take longer (ie: be in a higher tertile)

"It is a common belief among practitioners who do not study bias and efficiency in depth that the presence of non-linearity should be dealt with by chopping continuous variables into intervals. Nothing could be more disas-
trous."

BACK TO POISSON?!?!?
Am I crazy? Maybe. So the thing I was doing incorrectly with poisson before was throwing all the mdex times in as the outcome. For it to be analyzed as poisson, the outcome would need to be counts. This would mean creating catergories of length of mdex times and counting how many occured in that category. 

```{r}
#find out where 99.9% of the data happens, this is so I can just stop binning at a certain point
quantile(wb_data$mdex, probs = 0.999)

wb_data$mdex.counts <- cut(wb_data$mdex, breaks = c(-Inf, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200, 2300, 2400, 2500, 2600, 2700, 2800, 2900, 3000, Inf), labels = c("0:100", "101:200", "201:300", "301:400", "401:500", "501:600", "601:700", "701:800", "801:900", "901:1000", "1001:1100", "1101:1200", "1201:1300", "1301:1400", "1401:1500", "1501:1600",  "1601:1700", "1701:1800", "1801:1900", "1901:2000", "2001:2100", "2101:2200", "2201:2300", "2301:2400", "2401:2500", "2501:2600",  "2601:2700", "2701:2800", "2801:2900", "2901:3000","Over 3000"))

#try making the categories as integers
wb_data$mdex.counts <- cut(wb_data$mdex, breaks = c(-Inf, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1100, 1200, 1300, 1400, 1500, 1600, 1700, 1800, 1900, 2000, 2100, 2200, 2300, 2400, 2500, 2600, 2700, 2800, 2900, 3000, Inf), labels = c(1:31))

subset(wb_data, mdex == 101)

describe(wb_data$mdex.counts)
pois_data <- as.data.frame(table(wb_data$group, wb_data$mdex.counts))
names(pois_data)[1] <- "Group"
names(pois_data)[2] <- "Mdex_Time"
names(pois_data)[3] <- "Reports"
head(pois_data)

#average number of reports per time category for the pre group
mean(subset(pois_data, Group == 0)$Reports)

#rate of reports per time category overall 
sum(pois_data$Reports)/nrow(pois_data)


ggplot(data = pois_data, aes(x = Mdex_Time, y = Reports, group = Group, color = Group)) + geom_line() + geom_point() + theme(axis.text.x = element_text(angle = -45))

lambda.1 <- mean(subset(wb_data, group == 1)$mdex)
lambda.0 <- mean(subset(wb_data, group == 0)$mdex)

#oh snap, the sd is close to the mean
sd(subset(wb_data, group == 1)$mdex)
sd(subset(wb_data, group == 0)$mdex)
#oh wait.......it's the varaince that is supposed to equal the mean. So an order of magnitude off. Variance is much bigger
var(subset(wb_data, group == 1)$mdex)
var(subset(wb_data, group == 0)$mdex)

dpois((1:30)*100, lambda = lambda.1)
dpois((1:30)*100, lambda = lambda.0)

describe(pois_data)

str(pois_data)
pois_data$Mdex_Time <- as.numeric(pois_data$Mdex_Time)

#my intercept is the average number of reports per category. Is that even interesting? Or interpretable? My output is 
glm(data = pois_data, formula = Mdex_Time ~ Group, family = poisson)

nb.g.uni <- glm.nb(data = pois_data, formula = Mdex_Time ~ Group)
summary(nb.g.uni)
exp(nb.g.uni$coefficients)
exp(confint(nb.g.uni))
nb.g.uni


wb_data$mdex.counts <- as.numeric(wb_data$mdex.counts)
nb.g.uni <- glm.nb(data = wb_data, formula = mdex.counts ~ group)
summary(nb.g.uni)
exp(nb.g.uni$coefficients)
exp(confint(nb.g.uni))
nb.g.uni


```

